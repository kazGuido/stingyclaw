
# ─── Model config ────────────────────────────────────────────────────────────
# Stingyclaw auto-detects your backend from which keys are set:
#   GEMINI_API_KEY    → Gemini API direct (recommended, free)
#   OPENROUTER_API_KEY → OpenRouter (any model)
#   OPENROUTER_API_KEY=ollama → Local Ollama
#
# Priority: GEMINI_API_KEY takes precedence over OPENROUTER_API_KEY.

# ── Option 1: Gemini (recommended — free, fast, great tool use) ──
# Get a free key at https://aistudio.google.com → Get API Key
GEMINI_API_KEY=AIza...
# MODEL_NAME=gemini-2.5-flash          # default — fast, free
# MODEL_NAME=gemini-2.5-pro            # stronger, lower free tier limits

# ── Option 2: OpenRouter (any model) ──
# OPENROUTER_API_KEY=sk-or-v1-...
# MODEL_NAME=liquid/lfm-2.5            # free, good tool use
# MODEL_NAME=google/gemini-flash-1.5   # free tier
# MODEL_NAME=mistralai/mistral-7b-instruct:free
# OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ── Option 3: Local Ollama (fully offline) ──
# OPENROUTER_API_KEY=ollama
# MODEL_NAME=llama3.2
# OPENROUTER_BASE_URL=http://host.docker.internal:11434/v1

# ── Voice Service ──────────────────────────────────────────────────────────────
# URL of the local voice service (ASR + TTS). Runs as its own Docker container.
# Default is fine if running on the same machine.
# VOICE_SERVICE_URL=http://localhost:8001
